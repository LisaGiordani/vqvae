{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syaV08-n-MWJ"
   },
   "source": [
    "# Image generation with VQ-VAE-2 and GPT2 Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSK-d-tR96hI"
   },
   "source": [
    "\n",
    "\n",
    "Date : March 2022\n",
    "\n",
    "Author : Lisa Giordani\n",
    "\n",
    "Sources :\n",
    "-  \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:13:22.663145Z",
     "iopub.status.busy": "2022-03-09T08:13:22.662658Z",
     "iopub.status.idle": "2022-03-09T08:13:22.686827Z",
     "shell.execute_reply": "2022-03-09T08:13:22.686020Z",
     "shell.execute_reply.started": "2022-03-09T08:13:22.663056Z"
    },
    "id": "bKDRXRCovCq4"
   },
   "outputs": [],
   "source": [
    "env=\"cluster\" # \"kaggle\" or \"colab\" or \"cluster\"\n",
    "\n",
    "dataset=\"dog\" # \"dog\" or \"CIFAR\"\n",
    "generation_model=\"transformer\" # \"transformer\" or \"pixelCNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vVrsDzM6NO5e"
   },
   "source": [
    "## Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:13:22.688690Z",
     "iopub.status.busy": "2022-03-09T08:13:22.688363Z",
     "iopub.status.idle": "2022-03-09T08:13:39.586569Z",
     "shell.execute_reply": "2022-03-09T08:13:39.585642Z",
     "shell.execute_reply.started": "2022-03-09T08:13:22.688651Z"
    },
    "id": "LQjUUzi0dTrJ",
    "outputId": "d680e459-a7d4-43bd-fe9e-acab4e8d5719"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:13:39.589180Z",
     "iopub.status.busy": "2022-03-09T08:13:39.588337Z",
     "iopub.status.idle": "2022-03-09T08:13:43.898130Z",
     "shell.execute_reply": "2022-03-09T08:13:43.897372Z",
     "shell.execute_reply.started": "2022-03-09T08:13:39.589142Z"
    },
    "id": "QShRjMs7MyxF"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import cos, pi, floor, sin\n",
    "import xml.etree.ElementTree as ET \n",
    "from tqdm import tqdm_notebook\n",
    "from collections import defaultdict, OrderedDict\n",
    "import argparse\n",
    "from argparse import Namespace\n",
    "import itertools\n",
    "import pytorch_lightning as pl\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2LMHeadModel, GPT2Config\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda import is_available, device_count, get_device_name, current_device\n",
    "import torch.optim as optim \n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import shutil\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt, zipfile "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:13:43.901578Z",
     "iopub.status.busy": "2022-03-09T08:13:43.899413Z",
     "iopub.status.idle": "2022-03-09T08:13:43.912835Z",
     "shell.execute_reply": "2022-03-09T08:13:43.912113Z",
     "shell.execute_reply.started": "2022-03-09T08:13:43.901549Z"
    },
    "id": "KPvMXcFAqgjQ"
   },
   "outputs": [],
   "source": [
    "class Cudafy(object):\n",
    "\n",
    "    def __init__(self, device=None):\n",
    "        if is_available() and device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def check_devices():\n",
    "        for i in range(device_count()):\n",
    "            print(\"Found device {}:\".format(i), get_device_name(i))\n",
    "        if device_count() == 0:\n",
    "            print(\"No GPU device found\")\n",
    "        else:\n",
    "            print(\"Current cuda device is\", get_device_name(current_device()))\n",
    "\n",
    "    def name(self):\n",
    "        if is_available():\n",
    "            return get_device_name(self.device)\n",
    "        return 'Cuda is not available.'\n",
    "\n",
    "    def put(self, x):\n",
    "        \"\"\"Put x on the default cuda device.\"\"\"\n",
    "        if is_available():\n",
    "            return x.to(device=self.device)\n",
    "        return x\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.put(x)\n",
    "\n",
    "    def get(self, x):\n",
    "        \"\"\"Get from cpu.\"\"\"\n",
    "        if x.is_cuda:\n",
    "            return x.to(device='cpu')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:13:43.915777Z",
     "iopub.status.busy": "2022-03-09T08:13:43.914895Z",
     "iopub.status.idle": "2022-03-09T08:13:43.956519Z",
     "shell.execute_reply": "2022-03-09T08:13:43.955735Z",
     "shell.execute_reply.started": "2022-03-09T08:13:43.915738Z"
    },
    "id": "skbf8OLhp48w"
   },
   "outputs": [],
   "source": [
    "cudafy = Cudafy(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L9xjycgITFrV"
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:13:43.960285Z",
     "iopub.status.busy": "2022-03-09T08:13:43.960062Z",
     "iopub.status.idle": "2022-03-09T08:13:43.967834Z",
     "shell.execute_reply": "2022-03-09T08:13:43.967059Z",
     "shell.execute_reply.started": "2022-03-09T08:13:43.960251Z"
    },
    "id": "q1nFT3kR9PZL"
   },
   "outputs": [],
   "source": [
    "def load_images(batch_size, train=True):\n",
    "    while True:\n",
    "        for data, _ in create_data_loader(batch_size):\n",
    "            yield data\n",
    "\n",
    "\n",
    "def create_data_loader(batch_size):\n",
    "#     mnist = torchvision.datasets.MNIST('./data', train=train, download=True,\n",
    "#                                        transform=torchvision.transforms.ToTensor())\n",
    "    ds = torch.utils.data.TensorDataset(torch.Tensor(imagesIntorch), torch.Tensor(np.zeros(nb_images)))\n",
    "    return torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9lsOFFG45ZT"
   },
   "source": [
    "### Dog images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:13:43.971275Z",
     "iopub.status.busy": "2022-03-09T08:13:43.971030Z",
     "iopub.status.idle": "2022-03-09T08:13:43.978124Z",
     "shell.execute_reply": "2022-03-09T08:13:43.977471Z",
     "shell.execute_reply.started": "2022-03-09T08:13:43.971242Z"
    },
    "id": "FUsT2mwjTJdb",
    "outputId": "39dbd021-89d8-490f-da4b-7fa82f9e45cf"
   },
   "outputs": [],
   "source": [
    "if env==\"colab\" and dataset==\"dog\":\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    images_path = \"/content/drive/MyDrive/3A ENSTA/Projet IA/Bases de données/images.tar\"\n",
    "    annotation_path = \"/content/drive/MyDrive/3A ENSTA/Projet IA/Bases de données/annotation.tar\"\n",
    "\n",
    "    shutil.unpack_archive(images_path, \"content\")\n",
    "    shutil.unpack_archive(annotation_path, \"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:13:43.979723Z",
     "iopub.status.busy": "2022-03-09T08:13:43.979381Z",
     "iopub.status.idle": "2022-03-09T08:13:47.016339Z",
     "shell.execute_reply": "2022-03-09T08:13:47.015441Z",
     "shell.execute_reply.started": "2022-03-09T08:13:43.979667Z"
    },
    "id": "gW7TeP77TLjE"
   },
   "outputs": [],
   "source": [
    "if dataset==\"dog\":\n",
    "  if env==\"kaggle\":\n",
    "      ROOT = \"../input/dogimages/\" \n",
    "      ANNOTATION_PATH = ROOT + 'annotation/Annotation' \n",
    "      IMAGES_PATH = ROOT + 'images/Images' \n",
    "  elif env==\"colab\":\n",
    "      ROOT = \"content/\"\n",
    "      ANNOTATION_PATH = ROOT + 'Annotation' \n",
    "      IMAGES_PATH = ROOT + 'Images' \n",
    "  elif env==\"cluster\":\n",
    "      ROOT = \"data/\"\n",
    "      ANNOTATION_PATH = ROOT + 'Annotation' \n",
    "      IMAGES_PATH = ROOT + 'Images'\n",
    "      #if not \"data\" in os.listdir():\n",
    "      #  print(\"Extracting data...\")\n",
    "      #  shutil.unpack_archive(\"images.tar\", \"data\")\n",
    "      #  shutil.unpack_archive(\"annotation.tar\", \"data\")\n",
    "\n",
    "\n",
    "  IMAGES = {}\n",
    "  breeds = os.listdir(ANNOTATION_PATH) \n",
    "\n",
    "  compt =0\n",
    "  for breed in breeds:\n",
    "    IMAGES[breed] = []\n",
    "    for img in os.listdir(IMAGES_PATH + '/' + breed):\n",
    "      compt +=1\n",
    "      IMAGES[breed].append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:13:47.018424Z",
     "iopub.status.busy": "2022-03-09T08:13:47.017941Z",
     "iopub.status.idle": "2022-03-09T08:13:47.022420Z",
     "shell.execute_reply": "2022-03-09T08:13:47.021498Z",
     "shell.execute_reply.started": "2022-03-09T08:13:47.018388Z"
    },
    "id": "ftAHQg_mTOP0",
    "outputId": "62fb8a53-84aa-4f54-f2f4-4e4e38cb74b8"
   },
   "outputs": [],
   "source": [
    "if dataset==\"dog\":\n",
    "    compt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:13:47.024722Z",
     "iopub.status.busy": "2022-03-09T08:13:47.024110Z",
     "iopub.status.idle": "2022-03-09T08:17:44.229170Z",
     "shell.execute_reply": "2022-03-09T08:17:44.228560Z",
     "shell.execute_reply.started": "2022-03-09T08:13:47.024686Z"
    },
    "id": "qLZ2vyNOTP_o",
    "outputId": "ea916dd9-87ef-4dcc-dc7e-f8b4b88e7d7c"
   },
   "outputs": [],
   "source": [
    "if dataset==\"dog\":\n",
    "  ComputeLB = True\n",
    "  DogsOnly = True\n",
    "\n",
    "  IMAGES = {}\n",
    "  ALL_IMAGES = []\n",
    "  breeds = os.listdir(ANNOTATION_PATH) \n",
    "\n",
    "  for breed in breeds:\n",
    "    IMAGES[breed] = []\n",
    "    for img in os.listdir(IMAGES_PATH + '/' + breed):\n",
    "      IMAGES[breed].append(img)\n",
    "      ALL_IMAGES.append(img)\n",
    "\n",
    "  idxIn = 0; namesIn = []\n",
    "  imagesIn = np.zeros((25000,64,64,3))\n",
    "\n",
    "  # CROP WITH BOUNDING BOXES TO GET DOGS ONLY\n",
    "  # https://www.kaggle.com/paulorzp/show-annotations-and-breeds\n",
    "  if DogsOnly:\n",
    "      for breed in tqdm_notebook(breeds):\n",
    "          for dog in os.listdir(ANNOTATION_PATH + '/' + breed):\n",
    "              try: img = Image.open(IMAGES_PATH+'/'+breed+'/'+dog+'.jpg') \n",
    "              except: continue           \n",
    "              tree = ET.parse(ANNOTATION_PATH+'/'+breed+'/'+dog)\n",
    "              root = tree.getroot()\n",
    "              objects = root.findall('object')\n",
    "              for o in objects:\n",
    "                  bndbox = o.find('bndbox') \n",
    "                  xmin = int(bndbox.find('xmin').text)\n",
    "                  ymin = int(bndbox.find('ymin').text)\n",
    "                  xmax = int(bndbox.find('xmax').text)\n",
    "                  ymax = int(bndbox.find('ymax').text)\n",
    "                  w_, h_ = img.size\n",
    "                  w = np.max((xmax - xmin, ymax - ymin))\n",
    "                  img2 = img.crop((xmin, ymin, min(xmin+w, w_), min(ymin+w, h_)))\n",
    "                  img2 = img2.resize((64,64), Image.ANTIALIAS)\n",
    "                  if np.asarray(img2).shape == (64,64,3):\n",
    "                    imagesIn[idxIn,:,:,:] = np.asarray(img2)\n",
    "                  if idxIn%1000==0: print(idxIn)\n",
    "                  namesIn.append(breed)\n",
    "                  idxIn += 1\n",
    "      idx = np.arange(idxIn)\n",
    "      np.random.shuffle(idx)\n",
    "      imagesIn = imagesIn[idx,:,:,:]\n",
    "      namesIn = np.array(namesIn)[idx]\n",
    "      \n",
    "  # RANDOMLY CROP FULL IMAGES\n",
    "  else:\n",
    "      x = np.random.choice(np.arange(20579),10000)\n",
    "      for k in tqdm_notebook(range(len(x))):\n",
    "        for breed in breeds:\n",
    "          img = Image.open(IMAGES_PATH+\"/\"+breed+'/'+IMAGES[breed][0])\n",
    "          w = img.size[0]\n",
    "          h = img.size[1]\n",
    "          sz = np.min((w,h))\n",
    "          a=0; b=0\n",
    "          if w<h: b = (h-sz)//2\n",
    "          else: a = (w-sz)//2\n",
    "          img = img.crop((0+a, 0+b, sz+a, sz+b))  \n",
    "          img = img.resize((64,64), Image.ANTIALIAS)   \n",
    "          imagesIn[idxIn,:,:,:] = np.asarray(img)\n",
    "          namesIn.append(IMAGES[breed][0])\n",
    "          if idxIn%1000==0: print(idxIn)\n",
    "          idxIn += 1\n",
    "\n",
    "  print(idxIn)\n",
    "  # DISPLAY CROPPED IMAGES\n",
    "  x = np.random.randint(0,idxIn,25)\n",
    "  for k in range(5):\n",
    "      plt.figure(figsize=(15,3))\n",
    "      for j in range(5):\n",
    "          plt.subplot(1,5,j+1)\n",
    "          img = Image.fromarray( imagesIn[x[k*5+j],:,:,:].astype('uint8') )\n",
    "          plt.axis('off')\n",
    "          if not DogsOnly: plt.title(namesIn[x[k*5+j]],fontsize=11)\n",
    "          else: plt.title(namesIn[x[k*5+j]].split('-')[1],fontsize=11)\n",
    "          plt.imshow(img)\n",
    "      plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:17:44.232458Z",
     "iopub.status.busy": "2022-03-09T08:17:44.232076Z",
     "iopub.status.idle": "2022-03-09T08:17:44.238102Z",
     "shell.execute_reply": "2022-03-09T08:17:44.237126Z",
     "shell.execute_reply.started": "2022-03-09T08:17:44.232422Z"
    },
    "id": "5H-R94iJTVR-",
    "outputId": "df058629-0b9d-4625-f3dc-776fff91cb73"
   },
   "outputs": [],
   "source": [
    "if dataset==\"dog\":\n",
    "    len(breeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:17:44.240503Z",
     "iopub.status.busy": "2022-03-09T08:17:44.239738Z",
     "iopub.status.idle": "2022-03-09T08:17:46.581698Z",
     "shell.execute_reply": "2022-03-09T08:17:46.580911Z",
     "shell.execute_reply.started": "2022-03-09T08:17:44.240467Z"
    },
    "id": "EWGMCZAhTV_S",
    "outputId": "2715a91f-c9f6-4040-fb3e-34825d2f81da"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if dataset==\"dog\":\n",
    "    print(f'The shape of image is {imagesIn.shape}, the shape of imagename is {namesIn.shape}')\n",
    "    imagesIntorch = np.array([np.array(imagesIn[i]/255.0).transpose(2, 0, 1) for i in range(idxIn)])\n",
    "    print(f'The shape of reshaped image is {imagesIntorch.shape}')\n",
    "    dogs = list(set(namesIn))\n",
    "    len_dogs = len(dogs)\n",
    "    print(f'the number of dogs is {len_dogs}')\n",
    "    dog2id = {dogs[i]:i for i in range(len(dogs))}\n",
    "    id2dog = {v : k for k, v in dog2id.items()}\n",
    "    # print(dog2id, id2dog)\n",
    "    idIn = [dog2id[name] for name in namesIn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:17:46.583648Z",
     "iopub.status.busy": "2022-03-09T08:17:46.583212Z",
     "iopub.status.idle": "2022-03-09T08:17:46.587368Z",
     "shell.execute_reply": "2022-03-09T08:17:46.586689Z",
     "shell.execute_reply.started": "2022-03-09T08:17:46.583611Z"
    },
    "id": "4SCO73IBTjC8",
    "outputId": "3ecae46e-c0a9-4ee5-fcf1-b12d80c1cb73"
   },
   "outputs": [],
   "source": [
    "if dataset==\"dog\":\n",
    "    nb_images = imagesIntorch.shape[0]\n",
    "    imagesIntorch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o0QRVSEk6bbb"
   },
   "source": [
    "### CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:17:46.589036Z",
     "iopub.status.busy": "2022-03-09T08:17:46.588645Z",
     "iopub.status.idle": "2022-03-09T08:17:46.597632Z",
     "shell.execute_reply": "2022-03-09T08:17:46.596979Z",
     "shell.execute_reply.started": "2022-03-09T08:17:46.589000Z"
    }
   },
   "outputs": [],
   "source": [
    "if dataset==\"CIFAR\":\n",
    "    if env==\"colab\":\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        DATA_ROOT = \"/content/drive/MyDrive/3A ENSTA/Projet IA/vq-vae-2-pytorch-rosinality/data/cifar-10-batches-py/\"\n",
    "\n",
    "    if env==\"kaggle\":\n",
    "        DATA_ROOT = \"../input/cifar10/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:17:46.599559Z",
     "iopub.status.busy": "2022-03-09T08:17:46.599187Z",
     "iopub.status.idle": "2022-03-09T08:17:46.611152Z",
     "shell.execute_reply": "2022-03-09T08:17:46.610281Z",
     "shell.execute_reply.started": "2022-03-09T08:17:46.599512Z"
    }
   },
   "outputs": [],
   "source": [
    "if dataset==\"CIFAR\":\n",
    "    #!tar -xvf  '/content/drive/MyDrive/3A ENSTA/Projet IA/vq-vae-2-pytorch-rosinality/data/cifar-10-python.tar.gz' -C '/content/drive/MyDrive/3A ENSTA/Projet IA/vq-vae-2-pytorch-rosinality/data'\n",
    "\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "    data1 = unpickle(DATA_ROOT+'data_batch_1')\n",
    "    data2 = unpickle(DATA_ROOT+'data_batch_2')\n",
    "    data3 = unpickle(DATA_ROOT+'data_batch_3')\n",
    "    data4 = unpickle(DATA_ROOT+'data_batch_4')\n",
    "    data5 = unpickle(DATA_ROOT+'data_batch_5')\n",
    "    data6 = unpickle(DATA_ROOT+'test_batch')\n",
    "\n",
    "    data1 = data1[b'data']\n",
    "    data2 = data2[b'data']\n",
    "    data3 = data3[b'data']\n",
    "    data4 = data4[b'data']\n",
    "    data5 = data5[b'data']\n",
    "    data6 = data6[b'data']\n",
    "\n",
    "    data = np.concatenate((data1, data2), axis=0)\n",
    "    data = np.concatenate((data, data3), axis=0)\n",
    "    data = np.concatenate((data, data4), axis=0)\n",
    "    data = np.concatenate((data, data5), axis=0)\n",
    "    data = np.concatenate((data, data6), axis=0)\n",
    "\n",
    "    imagesIntorch = torch.tensor(data)\n",
    "    print(\"imagesIntorch\", imagesIntorch.shape)\n",
    "    imagesIntorch = imagesIntorch.reshape((imagesIntorch.shape[0], 3, 32, 32))\n",
    "    print(\"imagesIntorch\", imagesIntorch.shape)\n",
    "    imagesIntorch = imagesIntorch.float()\n",
    "    nb_images = imagesIntorch.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1EM5-9K8REiG"
   },
   "source": [
    "## General functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yxLNhFg9wDkn"
   },
   "source": [
    "### Vector-Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:17:46.612783Z",
     "iopub.status.busy": "2022-03-09T08:17:46.612524Z",
     "iopub.status.idle": "2022-03-09T08:17:46.639831Z",
     "shell.execute_reply": "2022-03-09T08:17:46.639127Z",
     "shell.execute_reply.started": "2022-03-09T08:17:46.612745Z"
    },
    "id": "Z0ssKU5vRSAD"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vector-Quantization for the VQ-VAE itself.\n",
    "\"\"\"\n",
    "\n",
    "def vq_loss(inputs, embedded, commitment=0.25):\n",
    "    \"\"\"\n",
    "    Compute the codebook and commitment losses for an\n",
    "    input-output pair from a VQ layer.\n",
    "    \"\"\"\n",
    "    return (torch.mean(torch.pow(inputs.detach() - embedded, 2)) +\n",
    "            commitment * torch.mean(torch.pow(inputs - embedded.detach(), 2)))\n",
    "\n",
    "\n",
    "class VQ(nn.Module):\n",
    "    \"\"\"\n",
    "    A vector quantization layer.\n",
    "    This layer takes continuous inputs and produces a few\n",
    "    different types of outputs, including a discretized\n",
    "    output, a commitment loss, a codebook loss, etc.\n",
    "    Args:\n",
    "        num_channels: the depth of the input Tensors.\n",
    "        num_latents: the number of latent values in the\n",
    "          dictionary to choose from.\n",
    "        dead_rate: the number of forward passes after\n",
    "          which a dictionary entry is considered dead if\n",
    "          it has not been used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels, num_latents, dead_rate=100):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.num_latents = num_latents\n",
    "        self.dead_rate = dead_rate\n",
    "\n",
    "        self.dictionary = nn.Parameter(torch.randn(num_latents, num_channels))\n",
    "        self.usage_count = nn.Parameter(dead_rate * torch.ones(num_latents).long(),\n",
    "                                        requires_grad=False)\n",
    "        self._last_batch = None\n",
    "\n",
    "    def embed(self, idxs):\n",
    "        \"\"\"\n",
    "        Convert encoded indices into embeddings.\n",
    "        Args:\n",
    "            idxs: an [N x H x W] or [N] Tensor.\n",
    "        Returns:\n",
    "            An [N x H x W x C] or [N x C] Tensor.\n",
    "        \"\"\"\n",
    "        embedded = F.embedding(idxs, self.dictionary)\n",
    "        if len(embedded.shape) == 4:\n",
    "            # NHWC to NCHW\n",
    "            embedded = embedded.permute(0, 3, 1, 2).contiguous()\n",
    "        return embedded\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Apply vector quantization.\n",
    "        If the module is in training mode, this will also\n",
    "        update the usage tracker and re-initialize dead\n",
    "        dictionary entries.\n",
    "        Args:\n",
    "            inputs: the input Tensor. Either [N x C] or\n",
    "              [N x C x H x W].\n",
    "        Returns:\n",
    "            A tuple (embedded, embedded_pt, idxs):\n",
    "              embedded: the new [N x C x H x W] Tensor\n",
    "                which passes gradients to the dictionary.\n",
    "              embedded_pt: like embedded, but with a\n",
    "                passthrough gradient estimator. Gradients\n",
    "                through this pass directly to the inputs.\n",
    "              idxs: a [N x H x W] Tensor of Longs\n",
    "                indicating the chosen dictionary entries.\n",
    "        \"\"\"\n",
    "        channels_last = inputs\n",
    "        if len(inputs.shape) == 4:\n",
    "            # NCHW to NHWC\n",
    "            channels_last = inputs.permute(0, 2, 3, 1).contiguous()\n",
    "\n",
    "        diffs = embedding_distances(self.dictionary, channels_last)\n",
    "        idxs = torch.argmin(diffs, dim=-1)\n",
    "        embedded = self.embed(idxs)\n",
    "        embedded_pt = embedded.detach() + (inputs - inputs.detach())\n",
    "\n",
    "        if self.training:\n",
    "            self._update_tracker(idxs)\n",
    "            self._last_batch = channels_last.detach()\n",
    "\n",
    "        return embedded, embedded_pt, idxs\n",
    "\n",
    "    def revive_dead_entries(self, inputs=None):\n",
    "        \"\"\"\n",
    "        Use the dictionary usage tracker to re-initialize\n",
    "        entries that aren't being used often.\n",
    "        Args:\n",
    "          inputs: a batch of inputs from which random\n",
    "            values are sampled for new entries. If None,\n",
    "            the previous input to forward() is used.\n",
    "        \"\"\"\n",
    "        if inputs is None:\n",
    "            assert self._last_batch is not None, ('cannot revive dead entries until a batch has ' +\n",
    "                                                  'been run')\n",
    "            inputs = self._last_batch\n",
    "        counts = self.usage_count.detach().cpu().numpy()\n",
    "        new_dictionary = None\n",
    "        inputs_numpy = None\n",
    "        for i, count in enumerate(counts):\n",
    "            if count:\n",
    "                continue\n",
    "            if new_dictionary is None:\n",
    "                new_dictionary = self.dictionary.detach().cpu().numpy()\n",
    "            if inputs_numpy is None:\n",
    "                inputs_numpy = inputs.detach().cpu().numpy().reshape([-1, inputs.shape[-1]])\n",
    "            new_dictionary[i] = random.choice(inputs_numpy)\n",
    "            counts[i] = self.dead_rate\n",
    "        if new_dictionary is not None:\n",
    "            dict_tensor = torch.from_numpy(new_dictionary).to(self.dictionary.device)\n",
    "            counts_tensor = torch.from_numpy(counts).to(self.usage_count.device)\n",
    "            self.dictionary.data.copy_(dict_tensor)\n",
    "            self.usage_count.data.copy_(counts_tensor)\n",
    "\n",
    "    def _update_tracker(self, idxs):\n",
    "        raw_idxs = set(idxs.detach().cpu().numpy().flatten())\n",
    "        update = -np.ones([self.num_latents], dtype=np.int)\n",
    "        for idx in raw_idxs:\n",
    "            update[idx] = self.dead_rate\n",
    "        self.usage_count.data.add_(torch.from_numpy(update).to(self.usage_count.device).long())\n",
    "        self.usage_count.data.clamp_(0, self.dead_rate)\n",
    "\n",
    "\n",
    "def embedding_distances(dictionary, tensor):\n",
    "    \"\"\"\n",
    "    Compute distances between every embedding in a\n",
    "    dictionary and every vector in a Tensor.\n",
    "    This will not generate a huge intermediate Tensor,\n",
    "    unlike the naive implementation.\n",
    "    Args:\n",
    "        dictionary: a [D x C] Tensor.\n",
    "        tensor: a [... x C] Tensor.\n",
    "    Returns:\n",
    "        A [... x D] Tensor of distances.\n",
    "    \"\"\"\n",
    "    dict_norms = torch.sum(torch.pow(dictionary, 2), dim=-1)\n",
    "    tensor_norms = torch.sum(torch.pow(tensor, 2), dim=-1)\n",
    "\n",
    "    # Work-around for https://github.com/pytorch/pytorch/issues/18862.\n",
    "    exp_tensor = tensor[..., None].view(-1, tensor.shape[-1], 1)\n",
    "    exp_dict = dictionary[None].expand(exp_tensor.shape[0], *dictionary.shape)\n",
    "    dots = torch.bmm(exp_dict, exp_tensor)[..., 0]\n",
    "    dots = dots.view(*tensor.shape[:-1], dots.shape[-1])\n",
    "\n",
    "    return -2 * dots + dict_norms + tensor_norms[..., None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PN1mOSLSwJbC"
   },
   "source": [
    "### Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:17:46.642030Z",
     "iopub.status.busy": "2022-03-09T08:17:46.641797Z",
     "iopub.status.idle": "2022-03-09T08:17:46.657445Z",
     "shell.execute_reply": "2022-03-09T08:17:46.656587Z",
     "shell.execute_reply.started": "2022-03-09T08:17:46.642005Z"
    },
    "id": "XcexEMGsRD9S"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    An abstract VQ-VAE encoder, which takes input Tensors,\n",
    "    shrinks them, and quantizes the result.\n",
    "    Sub-classes should overload the encode() method.\n",
    "    Args:\n",
    "        num_channels: the number of channels in the latent\n",
    "          codebook.\n",
    "        num_latents: the number of entries in the latent\n",
    "          codebook.\n",
    "        kwargs: arguments to pass to the VQ layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels, num_latents, **kwargs):\n",
    "        super().__init__()\n",
    "        self.vq = VQ(num_channels, num_latents, **kwargs)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode a Tensor before the VQ layer.\n",
    "        Args:\n",
    "            x: the input Tensor.\n",
    "        Returns:\n",
    "            A Tensor with the correct number of output\n",
    "              channels (according to self.vq).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Apply the encoder.\n",
    "        See VQ.forward() for return values.\n",
    "        \"\"\"\n",
    "        return self.vq(self.encode(x))\n",
    "\n",
    "\n",
    "class QuarterEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    The encoder from the original VQ-VAE paper that cuts\n",
    "    the dimensions down by a factor of 4 in both\n",
    "    directions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_latents, **kwargs):\n",
    "        super().__init__(out_channels, num_latents, **kwargs)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 4, stride=2)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 4, stride=2)\n",
    "        self.residual1 = _make_residual(out_channels)\n",
    "        self.residual2 = _make_residual(out_channels)\n",
    "\n",
    "    def encode(self, x):\n",
    "        # Padding is uneven, so we make the right and\n",
    "        # bottom more padded arbitrarily.\n",
    "        x = F.pad(x, (1, 2, 1, 2))\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.pad(x, (1, 2, 1, 2))\n",
    "        x = self.conv2(x)\n",
    "        x = x + self.residual1(x)\n",
    "        x = x + self.residual2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HalfEncoder(Encoder):\n",
    "    \"\"\"\n",
    "    An encoder that cuts the input size in half in both\n",
    "    dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, num_latents, **kwargs):\n",
    "        super().__init__(out_channels, num_latents, **kwargs)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 3, stride=2, padding=1)\n",
    "        self.residual1 = _make_residual(out_channels)\n",
    "        self.residual2 = _make_residual(out_channels)\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x + self.residual1(x)\n",
    "        x = x + self.residual2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itinp0y-wxQy"
   },
   "source": [
    "### Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:17:46.659164Z",
     "iopub.status.busy": "2022-03-09T08:17:46.658883Z",
     "iopub.status.idle": "2022-03-09T08:17:46.686703Z",
     "shell.execute_reply": "2022-03-09T08:17:46.685288Z",
     "shell.execute_reply.started": "2022-03-09T08:17:46.659131Z"
    },
    "id": "OmMv14Exwu_K"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    An abstract VQ-VAE decoder, which takes a stack of\n",
    "    (differently-sized) input Tensors and produces a\n",
    "    predicted output Tensor.\n",
    "    Sub-classes should overload the forward() method.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Apply the decoder to a list of inputs.\n",
    "        Args:\n",
    "            inputs: a sequence of input Tensors. There may\n",
    "              be more than one in the case of a hierarchy,\n",
    "              in which case the top levels come first.\n",
    "        Returns:\n",
    "            A decoded Tensor.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class QuarterDecoder(Decoder):\n",
    "    \"\"\"\n",
    "    The decoder from the original VQ-VAE paper that\n",
    "    upsamples the dimensions by a factor of 4 in both\n",
    "    directions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.residual1 = _make_residual(in_channels)\n",
    "        self.residual2 = _make_residual(in_channels)\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels, in_channels, 4, stride=2, padding=1)\n",
    "        self.conv2 = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        assert len(inputs) == 1\n",
    "        x = inputs[0]\n",
    "        x = x + self.residual1(x)\n",
    "        x = x + self.residual2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HalfDecoder(Decoder):\n",
    "    \"\"\"\n",
    "    A decoder that upsamples by a factor of 2 in both\n",
    "    dimensions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.residual1 = _make_residual(in_channels)\n",
    "        self.residual2 = _make_residual(in_channels)\n",
    "        self.conv = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        assert len(inputs) == 1\n",
    "        #print(\"inputs[0] HalfEncoder\",inputs[0].shape)\n",
    "        x = inputs[0]\n",
    "        x = x + self.residual1(x)\n",
    "        x = x + self.residual2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HalfQuarterDecoder(Decoder):\n",
    "    \"\"\"\n",
    "    A decoder that takes two inputs. The first one is\n",
    "    upsampled by a factor of two, and then combined with\n",
    "    the second input which is further upsampled by a\n",
    "    factor of four.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.residual1 = _make_residual(in_channels)\n",
    "        self.residual2 = _make_residual(in_channels)\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels, in_channels, 4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels * 2, in_channels, 3, padding=1)\n",
    "        self.residual3 = _make_residual(in_channels)\n",
    "        self.residual4 = _make_residual(in_channels)\n",
    "        self.conv3 = nn.ConvTranspose2d(in_channels, in_channels, 4, stride=2, padding=1)\n",
    "        self.conv4 = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        assert len(inputs) == 2\n",
    "        #print(\"inputs[0] HalfQuarterEncoder\",inputs[0].shape)\n",
    "        #print(\"inputs[1] HalfQuarterEncoder\",inputs[1].shape)\n",
    "        \n",
    "        # Upsample the top input to match the shape of the\n",
    "        # bottom input.\n",
    "        x = inputs[0]\n",
    "        x = x + self.residual1(x)\n",
    "        x = x + self.residual2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Mix together the bottom and top inputs.\n",
    "        x = torch.cat([x, inputs[1]], dim=1)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = x + self.residual3(x)\n",
    "        x = x + self.residual4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        return x\n",
    "\n",
    "class HalfHalfDecoder(Decoder):\n",
    "    \"\"\"\n",
    "    A decoder that takes two inputs. The first one is\n",
    "    upsampled by a factor of two, and then combined with\n",
    "    the second input which is further upsampled by a\n",
    "    factor of two.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.residual1 = _make_residual(in_channels)\n",
    "        self.residual2 = _make_residual(in_channels)\n",
    "        self.conv1 = nn.ConvTranspose2d(in_channels, in_channels, 4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels * 2, in_channels, 3, padding=1)\n",
    "        self.residual3 = _make_residual(in_channels)\n",
    "        self.residual4 = _make_residual(in_channels)\n",
    "        self.conv3 = nn.ConvTranspose2d(in_channels, out_channels, 4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        assert len(inputs) == 2\n",
    "        #print(\"inputs[0] HalfQuarterEncoder\",inputs[0].shape)\n",
    "        #print(\"inputs[1] HalfQuarterEncoder\",inputs[1].shape)\n",
    "        # Upsample the top input to match the shape of the\n",
    "        # bottom input.\n",
    "        x = inputs[0]\n",
    "        x = x + self.residual1(x)\n",
    "        x = x + self.residual2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Mix together the bottom and top inputs.\n",
    "        x = torch.cat([x, inputs[1]], dim=1)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        x = x + self.residual3(x)\n",
    "        x = x + self.residual4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eMiM9Aaww5Ea"
   },
   "source": [
    "### VQ-VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:17:46.688508Z",
     "iopub.status.busy": "2022-03-09T08:17:46.688293Z",
     "iopub.status.idle": "2022-03-09T08:17:46.709033Z",
     "shell.execute_reply": "2022-03-09T08:17:46.708432Z",
     "shell.execute_reply.started": "2022-03-09T08:17:46.688483Z"
    },
    "id": "_Mnrz6_Hw3jt"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An implementation of the hierarchical VQ-VAE.\n",
    "See https://arxiv.org/abs/1906.00446.\n",
    "\"\"\"\n",
    "\n",
    "class VQVAE(nn.Module):\n",
    "    \"\"\"\n",
    "    A complete VQ-VAE hierarchy.\n",
    "    There are N encoders, stored from the bottom level to\n",
    "    the top level, and N decoders stored from top to\n",
    "    bottom.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoders, decoders):\n",
    "        super().__init__()\n",
    "        assert len(encoders) == len(decoders)\n",
    "        self.encoders = encoders\n",
    "        self.decoders = decoders\n",
    "        for i, enc in enumerate(encoders):\n",
    "            self.add_module('encoder_%d' % i, enc)\n",
    "        for i, dec in enumerate(decoders):\n",
    "            self.add_module('decoder_%d' % i, dec)\n",
    "\n",
    "    def forward(self, inputs, commitment=0.25):\n",
    "        \"\"\"\n",
    "        Compute training losses for a batch of inputs.\n",
    "        Args:\n",
    "            inputs: the input Tensor. If this is a Tensor\n",
    "              of integers, then cross-entropy loss will be\n",
    "              used for the final decoder. Otherwise, MSE\n",
    "              will be used.\n",
    "            commitment: the commitment loss coefficient.\n",
    "        Returns:\n",
    "            A dict of Tensors, containing at least:\n",
    "              loss: the total training loss.\n",
    "              losses: the MSE/log-loss from each decoder.\n",
    "              reconstructions: a reconstruction Tensor\n",
    "                from each decoder.\n",
    "              embedded: outputs from every encoder, passed\n",
    "                through the vector-quantization table.\n",
    "                Ordered from bottom to top level.\n",
    "        \"\"\"\n",
    "        all_encoded = [inputs]\n",
    "        all_vq_outs = []\n",
    "        total_vq_loss = 0.0\n",
    "        total_recon_loss = 0.0\n",
    "        for encoder in self.encoders:\n",
    "            encoded = encoder.encode(all_encoded[-1])\n",
    "            embedded, embedded_pt, _ = encoder.vq(encoded)\n",
    "            all_encoded.append(encoded)\n",
    "            all_vq_outs.append(embedded_pt)\n",
    "            total_vq_loss = total_vq_loss + vq_loss(encoded, embedded, commitment=commitment)\n",
    "        losses = []\n",
    "        reconstructions = []\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            dec_inputs = all_vq_outs[::-1][:i + 1]\n",
    "            target = all_encoded[::-1][i + 1]\n",
    "            recon = decoder(dec_inputs)\n",
    "            reconstructions.append(recon)\n",
    "            if target.dtype.is_floating_point:\n",
    "                recon_loss = torch.mean(torch.pow(recon - target.detach(), 2))\n",
    "            else:\n",
    "                recon_loss = F.cross_entropy(recon.view(-1, recon.shape[-1]), target.view(-1))\n",
    "            total_recon_loss = total_recon_loss + recon_loss\n",
    "            losses.append(recon_loss)\n",
    "        return {\n",
    "            'loss': total_vq_loss + total_recon_loss,\n",
    "            'losses': losses,\n",
    "            'reconstructions': reconstructions,\n",
    "            'embedded': all_vq_outs,\n",
    "        }\n",
    "\n",
    "    def revive_dead_entries(self):\n",
    "        \"\"\"\n",
    "        Revive dead entries from all of the VQ layers.\n",
    "        Only call this once the encoders have all been\n",
    "        through a forward pass in training mode.\n",
    "        \"\"\"\n",
    "        for enc in self.encoders:\n",
    "            enc.vq.revive_dead_entries()\n",
    "\n",
    "    def full_reconstructions(self, inputs):\n",
    "        \"\"\"\n",
    "        Compute reconstructions of the inputs using all\n",
    "        the different layers of the hierarchy.\n",
    "        The first reconstruction uses only information\n",
    "        from the top-level codes, the second uses only\n",
    "        information from the top-level and second-to-top\n",
    "        level codes, etc.\n",
    "        This is not forward(inputs)['reconstructions'],\n",
    "        since said reconstructions are simply each level's\n",
    "        reconstruction of the next level's features.\n",
    "        Instead, full_reconstructions reconstructs the\n",
    "        original inputs.\n",
    "        \"\"\"\n",
    "        terms = self(inputs)\n",
    "        layer_recons = []\n",
    "        for encoder, recon in zip(self.encoders[:-1][::-1], terms['reconstructions'][:-1]):\n",
    "            _, embedded_pt, _ = encoder.vq(recon)\n",
    "            layer_recons.append(embedded_pt)\n",
    "        hierarchy_size = len(self.decoders)\n",
    "        results = []\n",
    "        for i in range(hierarchy_size - 1):\n",
    "            num_actual = i + 1\n",
    "            dec_in = terms['embedded'][-num_actual:][::-1] + layer_recons[num_actual - 1:]\n",
    "            results.append(self.decoders[-1](dec_in))\n",
    "        results.append(terms['reconstructions'][-1])\n",
    "        return results\n",
    "\n",
    "\n",
    "def _make_residual(channels):\n",
    "    return nn.Sequential(\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(channels, channels, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(channels, channels, 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPoMAK0IwR3m"
   },
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:17:46.710636Z",
     "iopub.status.busy": "2022-03-09T08:17:46.710395Z",
     "iopub.status.idle": "2022-03-09T08:17:46.763495Z",
     "shell.execute_reply": "2022-03-09T08:17:46.762754Z",
     "shell.execute_reply.started": "2022-03-09T08:17:46.710604Z"
    },
    "id": "yrMh0jHHYQ7b"
   },
   "outputs": [],
   "source": [
    "class CosineLR(lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, lr_min, lr_max, step_size):\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "        self.step_size = step_size\n",
    "        self.iteration = 0\n",
    "\n",
    "        super().__init__(optimizer, -1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr = self.lr_min + 0.5 * (self.lr_max - self.lr_min) * (\n",
    "            1 + cos(self.iteration / self.step_size * pi)\n",
    "        )\n",
    "        self.iteration += 1\n",
    "\n",
    "        if self.iteration == self.step_size:\n",
    "            self.iteration = 0\n",
    "\n",
    "        return [lr for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class PowerLR(lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, lr_min, lr_max, warmup):\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "        self.warmup = warmup\n",
    "        self.iteration = 0\n",
    "\n",
    "        super().__init__(optimizer, -1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.iteration < self.warmup:\n",
    "            lr = (\n",
    "                self.lr_min + (self.lr_max - self.lr_min) / self.warmup * self.iteration\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            lr = self.lr_max * (self.iteration - self.warmup + 1) ** -0.5\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "        return [lr for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class SineLR(lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, lr_min, lr_max, step_size):\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "        self.step_size = step_size\n",
    "        self.iteration = 0\n",
    "\n",
    "        super().__init__(optimizer, -1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr = self.lr_min + (self.lr_max - self.lr_min) * sin(\n",
    "            self.iteration / self.step_size * pi\n",
    "        )\n",
    "        self.iteration += 1\n",
    "\n",
    "        if self.iteration == self.step_size:\n",
    "            self.iteration = 0\n",
    "\n",
    "        return [lr for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class LinearLR(lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, lr_min, lr_max, warmup, step_size):\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "        self.step_size = step_size\n",
    "        self.warmup = warmup\n",
    "        self.iteration = 0\n",
    "\n",
    "        super().__init__(optimizer, -1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.iteration < self.warmup:\n",
    "            lr = self.lr_max\n",
    "\n",
    "        else:\n",
    "            lr = self.lr_max + (self.iteration - self.warmup) * (\n",
    "                self.lr_min - self.lr_max\n",
    "            ) / (self.step_size - self.warmup)\n",
    "        self.iteration += 1\n",
    "\n",
    "        if self.iteration == self.step_size:\n",
    "            self.iteration = 0\n",
    "\n",
    "        return [lr for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class CLR(lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, lr_min, lr_max, step_size):\n",
    "        self.epoch = 0\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_max = lr_max\n",
    "        self.current_lr = lr_min\n",
    "        self.step_size = step_size\n",
    "\n",
    "        super().__init__(optimizer, -1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        cycle = floor(1 + self.epoch / (2 * self.step_size))\n",
    "        x = abs(self.epoch / self.step_size - 2 * cycle + 1)\n",
    "        lr = self.lr_min + (self.lr_max - self.lr_min) * max(0, 1 - x)\n",
    "        self.current_lr = lr\n",
    "\n",
    "        self.epoch += 1\n",
    "\n",
    "        return [lr for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class Warmup(lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, model_dim, factor=1, warmup=16000):\n",
    "        self.optimizer = optimizer\n",
    "        self.model_dim = model_dim\n",
    "        self.factor = factor\n",
    "        self.warmup = warmup\n",
    "        self.iteration = 0\n",
    "\n",
    "        super().__init__(optimizer, -1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        self.iteration += 1\n",
    "        lr = (\n",
    "            self.factor\n",
    "            * self.model_dim ** (-0.5)\n",
    "            * min(self.iteration ** (-0.5), self.iteration * self.warmup ** (-1.5))\n",
    "        )\n",
    "\n",
    "        return [lr for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "# Copyright 2019 fastai\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\n",
    "# Borrowed from https://github.com/fastai/fastai and changed to make it runs like PyTorch lr scheduler\n",
    "\n",
    "\n",
    "class CycleAnnealScheduler:\n",
    "    def __init__(\n",
    "        self, optimizer, lr_max, lr_divider, cut_point, step_size, momentum=None\n",
    "    ):\n",
    "        self.lr_max = lr_max\n",
    "        self.lr_divider = lr_divider\n",
    "        self.cut_point = step_size // cut_point\n",
    "        self.step_size = step_size\n",
    "        self.iteration = 0\n",
    "        self.cycle_step = int(step_size * (1 - cut_point / 100) / 2)\n",
    "        self.momentum = momentum\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def get_lr(self):\n",
    "        if self.iteration > 2 * self.cycle_step:\n",
    "            cut = (self.iteration - 2 * self.cycle_step) / (\n",
    "                self.step_size - 2 * self.cycle_step\n",
    "            )\n",
    "            lr = self.lr_max * (1 + (cut * (1 - 100) / 100)) / self.lr_divider\n",
    "\n",
    "        elif self.iteration > self.cycle_step:\n",
    "            cut = 1 - (self.iteration - self.cycle_step) / self.cycle_step\n",
    "            lr = self.lr_max * (1 + cut * (self.lr_divider - 1)) / self.lr_divider\n",
    "\n",
    "        else:\n",
    "            cut = self.iteration / self.cycle_step\n",
    "            lr = self.lr_max * (1 + cut * (self.lr_divider - 1)) / self.lr_divider\n",
    "\n",
    "        return lr\n",
    "\n",
    "    def get_momentum(self):\n",
    "        if self.iteration > 2 * self.cycle_step:\n",
    "            momentum = self.momentum[0]\n",
    "\n",
    "        elif self.iteration > self.cycle_step:\n",
    "            cut = 1 - (self.iteration - self.cycle_step) / self.cycle_step\n",
    "            momentum = self.momentum[0] + cut * (self.momentum[1] - self.momentum[0])\n",
    "\n",
    "        else:\n",
    "            cut = self.iteration / self.cycle_step\n",
    "            momentum = self.momentum[0] + cut * (self.momentum[1] - self.momentum[0])\n",
    "\n",
    "        return momentum\n",
    "\n",
    "    def step(self):\n",
    "        lr = self.get_lr()\n",
    "\n",
    "        if self.momentum is not None:\n",
    "            momentum = self.get_momentum()\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "        if self.iteration == self.step_size:\n",
    "            self.iteration = 0\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] = lr\n",
    "\n",
    "            if self.momentum is not None:\n",
    "                group['betas'] = (momentum, group['betas'][1])\n",
    "\n",
    "        return lr\n",
    "\n",
    "\n",
    "def anneal_linear(start, end, proportion):\n",
    "    return start + proportion * (end - start)\n",
    "\n",
    "\n",
    "def anneal_cos(start, end, proportion):\n",
    "    cos_val = cos(pi * proportion) + 1\n",
    "\n",
    "    return end + (start - end) / 2 * cos_val\n",
    "\n",
    "\n",
    "class Phase:\n",
    "    def __init__(self, start, end, n_iter, anneal_fn):\n",
    "        self.start, self.end = start, end\n",
    "        self.n_iter = n_iter\n",
    "        self.anneal_fn = anneal_fn\n",
    "        self.n = 0\n",
    "\n",
    "    def step(self):\n",
    "        self.n += 1\n",
    "\n",
    "        return self.anneal_fn(self.start, self.end, self.n / self.n_iter)\n",
    "\n",
    "    def reset(self):\n",
    "        self.n = 0\n",
    "\n",
    "    @property\n",
    "    def is_done(self):\n",
    "        return self.n >= self.n_iter\n",
    "\n",
    "\n",
    "class CycleScheduler:\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer,\n",
    "        lr_max,\n",
    "        n_iter,\n",
    "        momentum=(0.95, 0.85),\n",
    "        divider=25,\n",
    "        warmup_proportion=0.3,\n",
    "        phase=('linear', 'cos'),\n",
    "    ):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        phase1 = int(n_iter * warmup_proportion)\n",
    "        phase2 = n_iter - phase1\n",
    "        lr_min = lr_max / divider\n",
    "\n",
    "        phase_map = {'linear': anneal_linear, 'cos': anneal_cos}\n",
    "\n",
    "        self.lr_phase = [\n",
    "            Phase(lr_min, lr_max, phase1, phase_map[phase[0]]),\n",
    "            Phase(lr_max, lr_min / 1e4, phase2, phase_map[phase[1]]),\n",
    "        ]\n",
    "\n",
    "        self.momentum = momentum\n",
    "\n",
    "        if momentum is not None:\n",
    "            mom1, mom2 = momentum\n",
    "            self.momentum_phase = [\n",
    "                Phase(mom1, mom2, phase1, phase_map[phase[0]]),\n",
    "                Phase(mom2, mom1, phase2, phase_map[phase[1]]),\n",
    "            ]\n",
    "\n",
    "        else:\n",
    "            self.momentum_phase = []\n",
    "\n",
    "        self.phase = 0\n",
    "\n",
    "    def step(self):\n",
    "        lr = self.lr_phase[self.phase].step()\n",
    "\n",
    "        if self.momentum is not None:\n",
    "            momentum = self.momentum_phase[self.phase].step()\n",
    "\n",
    "        else:\n",
    "            momentum = None\n",
    "\n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] = lr\n",
    "\n",
    "            if self.momentum is not None:\n",
    "                if 'betas' in group:\n",
    "                    group['betas'] = (momentum, group['betas'][1])\n",
    "\n",
    "                else:\n",
    "                    group['momentum'] = momentum\n",
    "\n",
    "        if self.lr_phase[self.phase].is_done:\n",
    "            self.phase += 1\n",
    "\n",
    "        if self.phase >= len(self.lr_phase):\n",
    "            for phase in self.lr_phase:\n",
    "                phase.reset()\n",
    "\n",
    "            for phase in self.momentum_phase:\n",
    "                phase.reset()\n",
    "\n",
    "            self.phase = 0\n",
    "\n",
    "        return lr, momentum\n",
    "\n",
    "\n",
    "class LRFinder(lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, lr_min, lr_max, step_size, linear=False):\n",
    "        ratio = lr_max / lr_min\n",
    "        self.linear = linear\n",
    "        self.lr_min = lr_min\n",
    "        self.lr_mult = (ratio / step_size) if linear else ratio ** (1 / step_size)\n",
    "        self.iteration = 0\n",
    "        self.lrs = []\n",
    "        self.losses = []\n",
    "\n",
    "        super().__init__(optimizer, -1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        lr = (\n",
    "            self.lr_mult * self.iteration\n",
    "            if self.linear\n",
    "            else self.lr_mult ** self.iteration\n",
    "        )\n",
    "        lr = self.lr_min + lr if self.linear else self.lr_min * lr\n",
    "\n",
    "        self.iteration += 1\n",
    "        self.lrs.append(lr)\n",
    "\n",
    "        return [lr for base_lr in self.base_lrs]\n",
    "\n",
    "    def record(self, loss):\n",
    "        self.losses.append(loss)\n",
    "\n",
    "    def save(self, filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            for lr, loss in zip(self.lrs, self.losses):\n",
    "                f.write('{},{}\\n'.format(lr, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LD4SXpQYYD-d"
   },
   "source": [
    "## Model VQ-VAE-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:17:46.766391Z",
     "iopub.status.busy": "2022-03-09T08:17:46.766174Z",
     "iopub.status.idle": "2022-03-09T08:17:46.775804Z",
     "shell.execute_reply": "2022-03-09T08:17:46.774931Z",
     "shell.execute_reply.started": "2022-03-09T08:17:46.766369Z"
    },
    "id": "hwsnXc0sYA-c"
   },
   "outputs": [],
   "source": [
    "def make_vae():\n",
    "    if dataset==\"dog\":\n",
    "        encoders = [QuarterEncoder(3, 64, 128), HalfEncoder(64, 64, 128)]\n",
    "        decoders = [HalfDecoder(64, 64), HalfQuarterDecoder(64, 3)]\n",
    "    if dataset==\"CIFAR\":\n",
    "        encoders = [HalfEncoder(3, 64, 128), HalfEncoder(64, 64, 128)]\n",
    "        decoders = [HalfDecoder(64, 64), HalfHalfDecoder(64, 3)]\n",
    "    return VQVAE(encoders, decoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qWqrS5HQdYB"
   },
   "source": [
    "## Training of VQ-VAE-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:17:46.777999Z",
     "iopub.status.busy": "2022-03-09T08:17:46.777107Z",
     "iopub.status.idle": "2022-03-09T08:17:46.785813Z",
     "shell.execute_reply": "2022-03-09T08:17:46.784985Z",
     "shell.execute_reply.started": "2022-03-09T08:17:46.777963Z"
    }
   },
   "outputs": [],
   "source": [
    "#!rm ./vae.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:17:46.788554Z",
     "iopub.status.busy": "2022-03-09T08:17:46.788147Z",
     "iopub.status.idle": "2022-03-09T08:27:42.413564Z",
     "shell.execute_reply": "2022-03-09T08:27:42.412900Z",
     "shell.execute_reply.started": "2022-03-09T08:17:46.788493Z"
    },
    "id": "wNQtG3k2QT-o",
    "outputId": "5dc6c795-7ff7-4dab-dc73-4d061598eaa5"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train a hierarchical VQ-VAE.\n",
    "\"\"\"\n",
    "\n",
    "VAE_PATH = 'vae.pt'\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def main_vae():\n",
    "    epochs = 5000 # init: 5000\n",
    "    batch_size = 256 #16\n",
    "    lr = 0.001 # init : 0.001\n",
    "    device = torch.device(DEVICE)\n",
    "    MSE = []\n",
    "\n",
    "    model = make_vae()\n",
    "    if os.path.exists(VAE_PATH):\n",
    "        model.load_state_dict(torch.load(VAE_PATH, map_location=DEVICE))\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    schedule = CycleScheduler(optimizer, lr, batch_size * epochs, momentum=(0.95, 0.85))\n",
    "    \n",
    "    data = load_images(batch_size = batch_size)\n",
    "    for i in itertools.count():\n",
    "        images = next(data).to(device)\n",
    "        terms = model(images)\n",
    "        mse, mse_top = terms['losses'][-1].item(), terms['losses'][0].item()\n",
    "        MSE.append(mse)\n",
    "        if i%50 == 0:\n",
    "            print('step %d: mse=%f mse_top=%f' %\n",
    "                (i, mse, mse_top))\n",
    "        optimizer.zero_grad()\n",
    "        terms['loss'].backward()\n",
    "        schedule.step()\n",
    "        optimizer.step()\n",
    "        model.revive_dead_entries()\n",
    "        \n",
    "        if i == epochs: \n",
    "            torch.save(model.state_dict(), VAE_PATH)\n",
    "            save_reconstructions(model, images)\n",
    "            break\n",
    "\n",
    "    # Display MSE\n",
    "    plt.plot(MSE)\n",
    "    plt.ylabel(\"Mean Squared Error (MSE)\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.title(\"Evolution of MSE through epochs\")\n",
    "    print(\"Final MSE :\", MSE[-1])\n",
    "\n",
    "    #return model\n",
    "\n",
    "def save_reconstructions(vae, images):\n",
    "    vae.eval()\n",
    "    with torch.no_grad():\n",
    "        recons = [torch.clamp(x, 0, 1).permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "                  for x in vae.full_reconstructions(images)]\n",
    "    vae.train()\n",
    "    top_recons, real_recons = recons\n",
    "    images = images.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "\n",
    "    columns = np.concatenate([top_recons, real_recons, images], axis=-2)\n",
    "    columns = np.concatenate(columns, axis=0)\n",
    "    Image.fromarray((columns * 255).astype('uint8')).save('reconstructions.png')\n",
    "\n",
    "main_vae()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8l0Mst2YyZ7"
   },
   "source": [
    "### Image reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:27:42.415108Z",
     "iopub.status.busy": "2022-03-09T08:27:42.414768Z",
     "iopub.status.idle": "2022-03-09T08:27:43.588368Z",
     "shell.execute_reply": "2022-03-09T08:27:43.584319Z",
     "shell.execute_reply.started": "2022-03-09T08:27:42.415071Z"
    },
    "id": "2vAHLx8rYo49",
    "outputId": "1d818cb9-22a4-4d8a-e1a3-35f3ee040ac6"
   },
   "outputs": [],
   "source": [
    "vae = Image.open('reconstructions.png')\n",
    "vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s4--_dEPXwvo"
   },
   "source": [
    "## Image generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_OPGlAcc2iv"
   },
   "source": [
    "### Transformer GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:27:43.590192Z",
     "iopub.status.busy": "2022-03-09T08:27:43.589784Z",
     "iopub.status.idle": "2022-03-09T08:27:43.631277Z",
     "shell.execute_reply": "2022-03-09T08:27:43.630617Z",
     "shell.execute_reply.started": "2022-03-09T08:27:43.590153Z"
    },
    "id": "5lrK7UNPXqbL"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "class Generator(pl.LightningModule):\n",
    "    def __init__(self, hparams, load_dataset=True):\n",
    "        super().__init__()\n",
    "        if load_dataset:\n",
    "            self.dataset = self.load_dataset(hparams)\n",
    "            hparams.vocab_size = self.dataset.vocab_size\n",
    "            hparams.height, hparams.width = self.dataset.shape[1:]\n",
    "            hparams.max_length = self.dataset.length\n",
    "            #hparams.start_token = self.dataset.start_token\n",
    "        else:\n",
    "            hparams.vocab_size = 256+64\n",
    "            hparams.height, hparams.width = 16+4,16\n",
    "            hparams.max_length = 256+64\n",
    "        self.model = self.build_model(hparams)\n",
    "        hparams = vars(hparams)\n",
    "        for key in hparams:\n",
    "            self.hparams[key] = hparams[key]\n",
    "        #self.hparams.update(vars(hparams))\n",
    "        #self.params = vars(hparams)\n",
    "        #self.max_length = hparams.max_length\n",
    "\n",
    "        \n",
    "    def load_dataset(self, hparams):\n",
    "        print(\"Loading the dataset of codes into memory...\")\n",
    "        device = \"cpu\"\n",
    "        \n",
    "        vae = make_vae()\n",
    "        vae.load_state_dict(torch.load(hparams.vqvae_model_path, map_location='cuda'))\n",
    "        vae.to(device)\n",
    "        vae.eval()\n",
    "\n",
    "        data = load_images(batch_size = hparams.batch_size)\n",
    "\n",
    "        bottom_codes = []\n",
    "        top_codes = []\n",
    "        #codes = []\n",
    "        nb = 0\n",
    "        for i in itertools.count():\n",
    "          print(\"i\",i)\n",
    "          print(\"nb\",nb)\n",
    "          images = next(data).to(device)\n",
    "          bottom_enc = vae.encoders[0].encode(images)\n",
    "          print(\"bottom_enc\", bottom_enc.shape)\n",
    "          _, _, bottom_idxs = vae.encoders[0].vq(bottom_enc)\n",
    "          _, _, top_idxs = vae.encoders[1](bottom_enc)\n",
    "          if nb==0:\n",
    "                print('bottom_idxs', bottom_idxs.shape)\n",
    "                print('top_idxs', top_idxs.shape)\n",
    "          bottom_codes.append(bottom_idxs.data.cpu())\n",
    "          top_codes.append(top_idxs.data.cpu())\n",
    "          nb += len(bottom_idxs) # batch size ou n_embed\n",
    "          if hasattr(hparams,'nb_examples') and hparams.nb_examples is not None and nb >= hparams.nb_examples:\n",
    "            break\n",
    "        bottom_codes = torch.cat(bottom_codes)\n",
    "        top_codes = torch.cat(top_codes)\n",
    "        print('Bottom code shape', bottom_codes.shape, type(bottom_codes),'min',bottom_codes.min(),'max', bottom_codes.max())\n",
    "        print('Top code shape', top_codes.shape, type(top_codes),'min',top_codes.min(),'max', top_codes.max())\n",
    "        #print(np.unique(codes))\n",
    "        if hparams.nb_examples and len(bottom_codes) >= hparams.nb_examples:\n",
    "            bottom_codes = bottom_codes[:hparams.nb_examples]\n",
    "            top_codes = top_codes[:hparams.nb_examples]\n",
    "\n",
    "        #vocab_size = vae.model.num_embeddings + 1\n",
    "        #start_token = vae.model.num_embeddings\n",
    "        #vocab_size =  256 #257, 65\n",
    "        #start_token = 256 #256, 64\n",
    "        bottom_codes_ = bottom_codes.view(len(bottom_codes), -1)\n",
    "        top_codes_ = top_codes.view(len(top_codes), -1)\n",
    "        codes_ = torch.cat((bottom_codes_,top_codes_),1)\n",
    "        print('codes_', codes_.shape)\n",
    "        #print('codes_', codes_)\n",
    "        codes_ = codes_.long()\n",
    "        #print('codes_ after long()', codes_)\n",
    "        #codes_ = torch.cat([(torch.ones(len(codes_), 1).long() * start_token), codes_.long(),], dim=1)\n",
    "        print('codes_', codes_.shape)\n",
    "\n",
    "        dataset = TensorDataset(codes_)\n",
    "        dataset.vocab_size = codes_.shape[1]\n",
    "        print('vocab_size', dataset.vocab_size)\n",
    "        dataset.shape = (bottom_codes.shape[0], int(bottom_codes.shape[1]+top_codes.shape[1]/2), bottom_codes.shape[2])\n",
    "        print('shape', dataset.shape)\n",
    "        dataset.length = codes_.shape[1]\n",
    "        print('length', dataset.length)\n",
    "        #dataset.start_token = start_token\n",
    "        #print('start_token', start_token)\n",
    "        print(\"Done loading dataset\")\n",
    "        \n",
    "        del codes_\n",
    "        #del codes\n",
    "        del bottom_codes\n",
    "        del top_codes\n",
    "        del bottom_idxs\n",
    "        del top_idxs\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def build_model(self, hparams):\n",
    "        config = GPT2Config(vocab_size=hparams.vocab_size, \n",
    "                            n_positions=hparams.max_length, \n",
    "                            n_ctx=hparams.max_length, \n",
    "                            n_embd=256, # 256\n",
    "                            n_layer=4, \n",
    "                            n_head=4, \n",
    "                            resid_pdrop=0.2,\n",
    "                            embd_pdrop=0.2,\n",
    "                            attn_pdrop=0.2)\n",
    "        return GPT2LMHeadModel(config)\n",
    "\n",
    "    def generate(self, nb_examples=1, **kwargs):\n",
    "        hparams = Namespace(**self.hparams)\n",
    "        input_ids = cudafy(torch.randint(0,self.hparams.vocab_size, (nb_examples, 1)).long())\n",
    "        #input_ids = torch.zeros(nb_examples, 1).long().to(self.device)\n",
    "        #input_ids[:] = hparams.vocab_size - 1\n",
    "        #input_ids[:] = hparams.vocab_size\n",
    "        result = self.model.generate(input_ids, max_length=hparams.max_length, **kwargs)\n",
    "        print(result.shape,'output of model')\n",
    "        #result = result[:, 1:]\n",
    "        result = result.contiguous()\n",
    "        print(result.shape,'output of model')\n",
    "        #print(result.shape, 'result after reshaping')\n",
    "        result = result.view(nb_examples, hparams.height, hparams.width)\n",
    "        print(result.shape,'output of model')\n",
    "        del input_ids\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return result\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        (X,) = batch\n",
    "        #print(\"X.shape\", X.shape)\n",
    "        outputs = self.model(X, labels=X)\n",
    "        loss = outputs.loss\n",
    "        #print(\"loss\", loss)\n",
    "        output = OrderedDict({\"loss\": loss, \"log\": {\"loss\": loss,},})\n",
    "        #print(\"output\", output)\n",
    "        del outputs, loss\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return output\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        hparams = Namespace(**self.hparams)\n",
    "        #del self.dataset\n",
    "        #torch.cuda.empty_cache()\n",
    "        #gc.collect()\n",
    "        return torch.utils.data.DataLoader(self.dataset, batch_size=hparams.batch_size, shuffle=True, num_workers=hparams.num_workers)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        hparams = Namespace(**self.hparams)\n",
    "        optimizer = optim.Adam(self.parameters(),lr=hparams.lr, weight_decay=hparams.weight_decay)\n",
    "        scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=hparams.scheduler_gamma)\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_2yvLIft_Xdt"
   },
   "source": [
    "### PixelCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:27:43.633251Z",
     "iopub.status.busy": "2022-03-09T08:27:43.632710Z",
     "iopub.status.idle": "2022-03-09T08:27:43.661020Z",
     "shell.execute_reply": "2022-03-09T08:27:43.660452Z",
     "shell.execute_reply.started": "2022-03-09T08:27:43.633203Z"
    },
    "id": "_WM_0YGs_W70",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An implementation of multi-head attention, based off of\n",
    "https://github.com/unixpickle/xformer\n",
    "\"\"\"\n",
    "\n",
    "class PixelAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention layer that operates on images.\n",
    "    Args:\n",
    "        num_channels: the input image depth.\n",
    "        num_heads: the number of attention heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.attention = MaskedAttention(num_channels, num_heads=num_heads)\n",
    "\n",
    "    def forward(self, *images, conds=None):\n",
    "        \"\"\"\n",
    "        Apply masked attention to a batch of images.\n",
    "        Args:\n",
    "            images: one or more [N x C x H x W] Tensors.\n",
    "            conds: ignored. Here for compatibility with\n",
    "              the PixelCNN aggregator.\n",
    "        Returns:\n",
    "            A new list of [N x C x H x W] Tensors.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for image in images:\n",
    "            batch, num_channels, height, width = image.shape\n",
    "            result = image.permute(0, 2, 3, 1)\n",
    "            result = result.view(batch, height * width, num_channels)\n",
    "            result = self.attention(result)\n",
    "            result = result.view(batch, height, width, num_channels)\n",
    "            result = result.permute(0, 3, 1, 2)\n",
    "            results.append(result + image)\n",
    "        if len(results) == 1:\n",
    "            return results[0]\n",
    "        return tuple(results)\n",
    "\n",
    "\n",
    "class MaskedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention layer that operates on sequences of the\n",
    "    shape [N x T x C], where N is the batch size, T is the\n",
    "    number of timesteps, and C is the number of channels.\n",
    "    Args:\n",
    "        num_channels: the number of channels in the input\n",
    "          sequences.\n",
    "        num_heads: the number of attention heads to use.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels, num_heads=8):\n",
    "        super().__init__()\n",
    "\n",
    "        assert not num_channels % num_heads, 'heads must evenly divide channels'\n",
    "        self.num_channels = num_channels\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.kqv_projection = nn.Linear(num_channels, num_channels * 3)\n",
    "        self.mix_heads = nn.Linear(num_channels, num_channels)\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        \"\"\"\n",
    "        Apply masked multi-head attention.\n",
    "        Args:\n",
    "            sequence: an [N x T x C] Tensor.\n",
    "        Returns:\n",
    "            A new [N x T x C] Tensor.\n",
    "        \"\"\"\n",
    "        projected = self.kqv_projection(sequence)\n",
    "        kqv = torch.split(projected, self.num_channels, dim=-1)\n",
    "        keys, queries, values = [self._split_heads(x) for x in kqv]\n",
    "        logits = torch.bmm(queries, keys.permute(0, 2, 1))\n",
    "        logits /= math.sqrt(self.num_channels / self.num_heads)\n",
    "        logits += self._logit_mask(sequence.shape[1])\n",
    "        weights = F.softmax(logits, dim=-1)\n",
    "        weighted_sum = torch.bmm(weights, values)\n",
    "        combined = self._combine_heads(weighted_sum)\n",
    "        return self.mix_heads(combined)\n",
    "\n",
    "    def _split_heads(self, batch):\n",
    "        \"\"\"\n",
    "        Split up the channels in a batch into groups, one\n",
    "        per head.\n",
    "        Args:\n",
    "            batch: an [N x T x C] Tensor.\n",
    "        Returns:\n",
    "            An [N*H x T x C/H] Tensor.\n",
    "        \"\"\"\n",
    "        batch_size = batch.shape[0]\n",
    "        num_steps = batch.shape[1]\n",
    "        split_channels = self.num_channels // self.num_heads\n",
    "        batch = batch.view(batch_size, num_steps, self.num_heads, split_channels)\n",
    "        batch = batch.permute(0, 2, 1, 3).contiguous()\n",
    "        batch = batch.view(batch_size * self.num_heads, num_steps, split_channels)\n",
    "        return batch\n",
    "\n",
    "    def _combine_heads(self, batch):\n",
    "        \"\"\"\n",
    "        Perform the inverse of _split_heads().\n",
    "        Args:\n",
    "            batch: an [N*H x T x C/H] Tensor.\n",
    "        Returns:\n",
    "            An [N x T x C] Tensor.\n",
    "        \"\"\"\n",
    "        batch_size = batch.shape[0] // self.num_heads\n",
    "        num_steps = batch.shape[1]\n",
    "        split_channels = self.num_channels // self.num_heads\n",
    "        batch = batch.view(batch_size, self.num_heads, num_steps, split_channels)\n",
    "        batch = batch.permute(0, 2, 1, 3).contiguous()\n",
    "        batch = batch.view(batch_size, num_steps, self.num_channels)\n",
    "        return batch\n",
    "\n",
    "    def _logit_mask(self, num_steps):\n",
    "        row_indices = np.arange(num_steps)[:, None]\n",
    "        col_indices = np.arange(num_steps)[None]\n",
    "        upper = (row_indices >= col_indices)\n",
    "        mask = np.where(upper, 0, -np.inf).astype(np.float32)\n",
    "        return torch.from_numpy(mask).to(next(self.parameters()).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:27:43.662941Z",
     "iopub.status.busy": "2022-03-09T08:27:43.662512Z",
     "iopub.status.idle": "2022-03-09T08:27:43.708463Z",
     "shell.execute_reply": "2022-03-09T08:27:43.707620Z",
     "shell.execute_reply.started": "2022-03-09T08:27:43.662906Z"
    },
    "id": "ktV0zawJ_6OP",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An implementation of the Gated PixelCNN from\n",
    "https://arxiv.org/abs/1606.05328.\n",
    "\"\"\"\n",
    "\n",
    "class PixelCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A PixelCNN is a stack of PixelConv layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        for i, layer in enumerate(layers):\n",
    "            self.add_module('layer_%d' % i, layer)\n",
    "        self.layers = layers\n",
    "\n",
    "    def forward(self, images, conds=None):\n",
    "        \"\"\"\n",
    "        Apply the stack of PixelConv layers.\n",
    "        It is assumed that the first layer is a\n",
    "        PixelConvA, and the rest are PixelConvB's.\n",
    "        This way, the first layer takes one input and the\n",
    "        rest take two.\n",
    "        Returns:\n",
    "            A tuple (vertical, horizontal), one for each\n",
    "              of the two directional stacks.\n",
    "        \"\"\"\n",
    "        outputs = self.layers[0](images, conds=conds)\n",
    "        for layer in self.layers[1:]:\n",
    "            outputs = layer(*outputs, conds=conds)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class PixelConv(nn.Module):\n",
    "    \"\"\"\n",
    "    An abstract base class for PixelCNN layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, depth_in, depth_out, cond_depth=None, horizontal=2, vertical=2):\n",
    "        super().__init__()\n",
    "        self.depth_in = depth_in\n",
    "        self.depth_out = depth_out\n",
    "        self.horizontal = horizontal\n",
    "        self.vertical = vertical\n",
    "\n",
    "        self._init_directional_convs()\n",
    "        self.vert_to_horiz = nn.Conv2d(depth_out * 2, depth_out * 2, 1)\n",
    "        self.cond_layer = None\n",
    "        if cond_depth is not None:\n",
    "            self.cond_layer = nn.Linear(cond_depth, depth_out * 4)\n",
    "\n",
    "    def _init_directional_convs(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _run_stacks(self, vert_in, horiz_in, conds):\n",
    "        vert_out = self._run_padded_vertical(vert_in)\n",
    "        horiz_out = self._run_padded_horizontal(horiz_in)\n",
    "        horiz_out = horiz_out + self.vert_to_horiz(vert_out)\n",
    "\n",
    "        if conds is not None:\n",
    "            cond_bias = self._compute_cond_bias(conds)\n",
    "            vert_out = vert_out + cond_bias[:, :self.depth_out*2]\n",
    "            horiz_out = horiz_out + cond_bias[:, self.depth_out*2:]\n",
    "\n",
    "        vert_out = gated_activation(vert_out)\n",
    "        horiz_out = gated_activation(horiz_out)\n",
    "        return vert_out, horiz_out\n",
    "\n",
    "    def _run_padded_vertical(self, vert_in):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _run_padded_horizontal(self, horiz_in):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _compute_cond_bias(self, conds):\n",
    "        if len(conds.shape) == 2:\n",
    "            outputs = self.cond_layer(conds)\n",
    "            return outputs.view(-1, outputs.shape[1], 1, 1)\n",
    "        assert len(conds.shape) == 4\n",
    "        conds_perm = conds.permute(0, 2, 3, 1)\n",
    "        outputs = self.cond_layer(conds_perm)\n",
    "        return outputs.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class PixelConvA(PixelConv):\n",
    "    \"\"\"\n",
    "    The first layer in a PixelCNN. This layer is unlike\n",
    "    the other layers, in that it does not allow the stack\n",
    "    to see the current pixel.\n",
    "    Args:\n",
    "        depth_in: the number of input filters.\n",
    "        depth_out: the number of output filters.\n",
    "        cond_depth: the number of conditioning channels.\n",
    "          If None, this is an unconditional model.\n",
    "        horizontal: the receptive field of the horizontal\n",
    "          stack.\n",
    "        vertical: the receptive field of the vertical\n",
    "          stack.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, depth_in, depth_out, cond_depth=None, horizontal=2, vertical=2):\n",
    "        super().__init__(depth_in, depth_out, cond_depth=cond_depth, horizontal=2, vertical=2)\n",
    "\n",
    "    def forward(self, images, conds=None):\n",
    "        \"\"\"\n",
    "        Apply the layer to some images, producing latents.\n",
    "        Args:\n",
    "            images: an NCHW batch of images.\n",
    "            conds: an optional conditioning value. If set,\n",
    "              either an NCHW Tensor or an NxM Tensor.\n",
    "        Returns:\n",
    "            A tuple (vertical, horizontal), one for each\n",
    "              of the two directional stacks.\n",
    "        \"\"\"\n",
    "        return self._run_stacks(images, images, conds)\n",
    "\n",
    "    def _init_directional_convs(self):\n",
    "        self.vertical_conv = nn.Conv2d(self.depth_in, self.depth_out * 2,\n",
    "                                       (self.vertical, self.horizontal*2 + 1))\n",
    "        self.horizontal_conv = nn.Conv2d(self.depth_in, self.depth_out * 2, (1, self.horizontal))\n",
    "\n",
    "    def _run_padded_vertical(self, vert_in):\n",
    "        vert_pad = (self.horizontal, self.horizontal, self.vertical, 0)\n",
    "        return self.vertical_conv(F.pad(vert_in, vert_pad))[:, :, :-1, :]\n",
    "\n",
    "    def _run_padded_horizontal(self, horiz_in):\n",
    "        return self.horizontal_conv(F.pad(horiz_in, (self.horizontal, 0, 0, 0)))[:, :, :, :-1]\n",
    "\n",
    "\n",
    "class PixelConvB(PixelConv):\n",
    "    \"\"\"\n",
    "    Any layer except the first in a PixelCNN.\n",
    "    Args:\n",
    "        depth_in: the number of input filters.\n",
    "        cond_depth: the number of conditioning channels.\n",
    "          If None, this is an unconditional model.\n",
    "        horizontal: the receptive field of the horizontal\n",
    "          stack.\n",
    "        vertical: the receptive field of the vertical\n",
    "          stack.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, depth_in, cond_depth=None, norm=False, horizontal=2, vertical=2):\n",
    "        super().__init__(depth_in, depth_in, cond_depth=cond_depth, horizontal=horizontal,\n",
    "                         vertical=vertical)\n",
    "        self.horiz_residual = nn.Conv2d(depth_in, depth_in, 1)\n",
    "        self.vert_norm = lambda x: x\n",
    "        self.horiz_norm = lambda x: x\n",
    "        if norm:\n",
    "            self.vert_norm = ChannelNorm(depth_in)\n",
    "            self.horiz_norm = ChannelNorm(depth_in)\n",
    "\n",
    "    def forward(self, vert_in, horiz_in, conds=None):\n",
    "        \"\"\"\n",
    "        Apply the layer to the outputs of previous\n",
    "        vertical and horizontal stacks.\n",
    "        Args:\n",
    "            vert_in: an NCHW Tensor.\n",
    "            horiz_in: an NCHW Tensor.\n",
    "            conds: an optional conditioning value. If set,\n",
    "              either an NCHW Tensor or an NxM Tensor.\n",
    "        Returns:\n",
    "            A tuple (vertical, horizontal), one for each\n",
    "              of the two directional stacks.\n",
    "        \"\"\"\n",
    "        vert_out, horiz_out = self._run_stacks(vert_in, horiz_in, conds)\n",
    "        horiz_out = horiz_in + self.horiz_norm(self.horiz_residual(horiz_out))\n",
    "        return self.vert_norm(vert_out), horiz_out\n",
    "\n",
    "    def _init_directional_convs(self):\n",
    "        self.vertical_conv = nn.Conv2d(self.depth_in, self.depth_out * 2,\n",
    "                                       (self.vertical + 1, self.horizontal*2 + 1))\n",
    "        self.horizontal_conv = nn.Conv2d(self.depth_in, self.depth_out * 2,\n",
    "                                         (1, self.horizontal + 1))\n",
    "\n",
    "    def _run_padded_vertical(self, vert_in):\n",
    "        vert_pad = (self.horizontal, self.horizontal, self.vertical, 0)\n",
    "        return self.vertical_conv(F.pad(vert_in, vert_pad))\n",
    "\n",
    "    def _run_padded_horizontal(self, horiz_in):\n",
    "        return self.horizontal_conv(F.pad(horiz_in, (self.horizontal, 0, 0, 0)))\n",
    "\n",
    "\n",
    "class ChannelNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    A layer which applies layer normalization to the\n",
    "    channels at each spacial location separately.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm((num_channels,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 3, 1).contiguous()\n",
    "        x = self.norm(x)\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        return x\n",
    "\n",
    "\n",
    "def gated_activation(outputs):\n",
    "    depth = outputs.shape[1] // 2\n",
    "    tanh = torch.tanh(outputs[:, :depth])\n",
    "    sigmoid = torch.sigmoid(outputs[:, depth:])\n",
    "    return tanh * sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:27:43.710617Z",
     "iopub.status.busy": "2022-03-09T08:27:43.709959Z",
     "iopub.status.idle": "2022-03-09T08:27:43.752068Z",
     "shell.execute_reply": "2022-03-09T08:27:43.750828Z",
     "shell.execute_reply.started": "2022-03-09T08:27:43.710584Z"
    },
    "id": "SrRu7XUOAxGw",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Models for hierarchical image generation.\n",
    "\"\"\"\n",
    "\n",
    "class TopPrior(nn.Module):\n",
    "    def __init__(self, depth=64, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(128, depth)\n",
    "        self.pixel_cnn = PixelCNN(\n",
    "            PixelConvA(depth, depth),\n",
    "\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelAttention(depth, num_heads=num_heads),\n",
    "\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelAttention(depth, num_heads=num_heads),\n",
    "\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelAttention(depth, num_heads=num_heads),\n",
    "\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelConvB(depth, norm=True),\n",
    "            PixelAttention(depth, num_heads=num_heads),\n",
    "        )\n",
    "        self.out_stack = nn.Sequential(\n",
    "            nn.Conv2d(depth * 2, depth, 1),\n",
    "            Residual1x1(depth),\n",
    "            Residual1x1(depth),\n",
    "            Residual1x1(depth),\n",
    "            Residual1x1(depth),\n",
    "            Residual1x1(depth),\n",
    "            Residual1x1(depth),\n",
    "            Residual1x1(depth),\n",
    "            Residual1x1(depth),\n",
    "            Residual1x1(depth),\n",
    "            Residual1x1(depth),\n",
    "            nn.Conv2d(depth, 256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = x.permute(0, 3, 1, 2).contiguous()\n",
    "        out1, out2 = self.pixel_cnn(x)\n",
    "        return self.out_stack(torch.cat([out1, out2], dim=1))\n",
    "\n",
    "\n",
    "class BottomPrior(nn.Module):\n",
    "    def __init__(self, depth=64, num_heads=2):\n",
    "        super().__init__()\n",
    "        self.embed_top = nn.Embedding(128, depth)\n",
    "        self.embed_bottom = nn.Embedding(128, depth)\n",
    "        self.cond_stack = nn.Sequential(\n",
    "            Residual3x3(depth),\n",
    "            Residual3x3(depth),\n",
    "            Residual3x3(depth),\n",
    "            Residual3x3(depth),\n",
    "            Residual3x3(depth),\n",
    "            Residual3x3(depth),\n",
    "            Residual3x3(depth),\n",
    "            Residual3x3(depth),\n",
    "            Residual3x3(depth),\n",
    "            nn.ConvTranspose2d(depth, depth, 4, stride=2, padding=1),\n",
    "        )\n",
    "        self.pixel_cnn = PixelCNN(\n",
    "            PixelConvA(depth, depth, cond_depth=depth),\n",
    "\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "            PixelConvB(depth, cond_depth=depth, norm=True),\n",
    "        )\n",
    "        self.out_stack = nn.Sequential(\n",
    "            nn.Conv2d(depth * 2, depth, 1),\n",
    "            nn.Conv2d(depth, 128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, bottom, top):\n",
    "        conds = self.embed_top(top)\n",
    "        conds = conds.permute(0, 3, 1, 2).contiguous()\n",
    "        conds = self.cond_stack(conds)\n",
    "\n",
    "        out = self.embed_bottom(bottom)\n",
    "        out = out.permute(0, 3, 1, 2).contiguous()\n",
    "        out1, out2 = self.pixel_cnn(out, conds=conds)\n",
    "        return self.out_stack(torch.cat([out1, out2], dim=1))\n",
    "\n",
    "\n",
    "class Residual1x1(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, num_channels, 1)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, 1)\n",
    "        self.norm = ChannelNorm(num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return inputs + self.norm(x)\n",
    "\n",
    "\n",
    "class Residual3x3(nn.Module):\n",
    "    def __init__(self, num_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, num_channels, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(num_channels, num_channels, 3, padding=1)\n",
    "        self.norm = ChannelNorm(num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = x\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        return inputs + self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pflzZBjDdAMR"
   },
   "source": [
    "### Training of generation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:27:43.756912Z",
     "iopub.status.busy": "2022-03-09T08:27:43.756355Z",
     "iopub.status.idle": "2022-03-09T08:28:47.924641Z",
     "shell.execute_reply": "2022-03-09T08:28:47.923807Z",
     "shell.execute_reply.started": "2022-03-09T08:27:43.756877Z"
    },
    "id": "AkjqUQ-mdGf1",
    "outputId": "a82363f7-9945-4ff0-9744-ecb951c3e103"
   },
   "outputs": [],
   "source": [
    "if generation_model==\"transformer\":\n",
    "    def train_transformer_generator(hparams):\n",
    "        torch.cuda.empty_cache()\n",
    "        hparams = Namespace(**hparams)\n",
    "        generator = Generator(hparams, load_dataset=True)\n",
    "        logger = pl.loggers.TensorBoardLogger(save_dir=hparams.folder, name=\"logs\")\n",
    "        trainer = pl.Trainer(#default_root_dir=hparams.folder, \n",
    "                             enable_checkpointing=False,\n",
    "                             #weights_save_path=hparams.folder+'/checkpoints',\n",
    "                             max_epochs=hparams.epochs,\n",
    "                             enable_progress_bar=True,\n",
    "                             gpus=hparams.gpus,\n",
    "                             logger=logger,\n",
    "                             #resume_from_checkpoint=hparams.resume_from_checkpoint,\n",
    "                             detect_anomaly=True)\n",
    "        trainer.fit(generator) #, ckpt_path=hparams.resume_from_checkpoint)\n",
    "        return generator, trainer\n",
    "\n",
    "    hparams = {'resume_from_checkpoint': None,\n",
    "            #'folder': '/out/generator', \n",
    "            'epochs': 5, #200  \n",
    "            'lr': 1e-3, \n",
    "            'weight_decay': 0, \n",
    "            'scheduler_gamma':  1, \n",
    "            'batch_size': 128, # ok : 32, 64, 128 / not ok : 256\n",
    "            'num_workers': 1, \n",
    "            'nb_examples':2000, #nb_images\n",
    "            'vqvae_model_path':VAE_PATH, \n",
    "            'gpus': 1 if torch.cuda.is_available() else 0}\n",
    "    generator, trainergenerator = train_transformer_generator(hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:28:47.926400Z",
     "iopub.status.busy": "2022-03-09T08:28:47.926132Z",
     "iopub.status.idle": "2022-03-09T08:28:47.938738Z",
     "shell.execute_reply": "2022-03-09T08:28:47.938095Z",
     "shell.execute_reply.started": "2022-03-09T08:28:47.926366Z"
    },
    "id": "1wPYYdUSBEge",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the top-level prior.\n",
    "\"\"\"\n",
    "\n",
    "if generation_model==\"pixelCNN\":\n",
    "  TOP_PRIOR_PATH = 'top.pt'\n",
    "\n",
    "  def main_top():\n",
    "      epochs = 10000 # 60000\n",
    "      batch_size = 64\n",
    "      lr = 3e-4\n",
    "      device = torch.device(DEVICE)\n",
    "      LOSS = []\n",
    "\n",
    "      vae = make_vae()\n",
    "      vae.load_state_dict(torch.load(VAE_PATH, map_location='cpu'))\n",
    "      vae.to(device)\n",
    "      vae.eval()\n",
    "\n",
    "      top_prior = TopPrior()\n",
    "      if os.path.exists(TOP_PRIOR_PATH):\n",
    "          top_prior.load_state_dict(torch.load(TOP_PRIOR_PATH, map_location='cuda'))\n",
    "      top_prior.to(device)\n",
    "\n",
    "      optimizer = optim.Adam(top_prior.parameters(), lr=lr)\n",
    "      loss_fn = nn.CrossEntropyLoss()\n",
    "      \n",
    "      schedule = CycleScheduler(optimizer, lr, batch_size * epochs, momentum=(0.95, 0.85),)\n",
    "      data = load_images(batch_size = batch_size)\n",
    "      for i in itertools.count():\n",
    "          images = next(data).to(device)\n",
    "          _, _, encoded = vae.encoders[1](vae.encoders[0].encode(images))\n",
    "          logits = top_prior(encoded)\n",
    "          logits = logits.permute(0, 2, 3, 1).contiguous()\n",
    "          logits = logits.view(-1, logits.shape[-1])\n",
    "          loss = loss_fn(logits, encoded.view(-1))\n",
    "          LOSS.append(loss)\n",
    "          if i%100 == 0:\n",
    "            print('step %d: loss=%f' % (i, loss.item()))\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          schedule.step()\n",
    "          optimizer.step()\n",
    "          if not i % 30:\n",
    "              torch.save(top_prior.state_dict(), TOP_PRIOR_PATH)\n",
    "              \n",
    "          if i == epochs: break\n",
    "      \n",
    "      # Display loss\n",
    "      plt.plot(LOSS)\n",
    "      plt.ylabel(\"Loss\")\n",
    "      plt.xlabel(\"Epochs\")\n",
    "      plt.title(\"Evolution of the loss through epochs\")\n",
    "\n",
    "\n",
    "  main_top()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:28:47.940481Z",
     "iopub.status.busy": "2022-03-09T08:28:47.940039Z",
     "iopub.status.idle": "2022-03-09T08:28:47.956422Z",
     "shell.execute_reply": "2022-03-09T08:28:47.955635Z",
     "shell.execute_reply.started": "2022-03-09T08:28:47.940440Z"
    },
    "id": "zV6MI4LBBOdA",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train the bottom-level prior.\n",
    "\"\"\"\n",
    "\n",
    "if generation_model==\"pixelCNN\":\n",
    "  BOTTOM_PRIOR_PATH = 'bottom.pt'\n",
    "\n",
    "  def main_bottom():\n",
    "      epochs = 4700 # 60000\n",
    "      batch_size = 64\n",
    "      lr = 3e-4\n",
    "      device = torch.device(DEVICE)\n",
    "      LOSS = []\n",
    "\n",
    "      vae = make_vae()\n",
    "      vae.load_state_dict(torch.load(VAE_PATH, map_location='cuda'))\n",
    "      vae.to(device)\n",
    "      vae.eval()\n",
    "\n",
    "      bottom_prior = BottomPrior()\n",
    "      if os.path.exists(BOTTOM_PRIOR_PATH):\n",
    "          bottom_prior.load_state_dict(torch.load(BOTTOM_PRIOR_PATH, map_location='cuda'))\n",
    "      bottom_prior.to(device)\n",
    "\n",
    "      optimizer = optim.Adam(bottom_prior.parameters(), lr=lr)\n",
    "      loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "      schedule = CycleScheduler(optimizer, lr, batch_size * epochs, momentum=(0.95, 0.85),)\n",
    "      data = load_images(batch_size = batch_size)\n",
    "      for i in itertools.count():\n",
    "          images = next(data).to(device)\n",
    "          bottom_enc = vae.encoders[0].encode(images)\n",
    "          _, _, bottom_idxs = vae.encoders[0].vq(bottom_enc)\n",
    "          _, _, top_idxs = vae.encoders[1](bottom_enc)\n",
    "          logits = bottom_prior(bottom_idxs, top_idxs)\n",
    "          logits = logits.permute(0, 2, 3, 1).contiguous()\n",
    "          logits = logits.view(-1, logits.shape[-1])\n",
    "          loss = loss_fn(logits, bottom_idxs.view(-1))\n",
    "          LOSS.append(loss)\n",
    "          if i%100 == 0:\n",
    "            print('step %d: loss=%f' % (i, loss.item()))\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          if not i % 30:\n",
    "              torch.save(bottom_prior.state_dict(), BOTTOM_PRIOR_PATH)\n",
    "          if i == epochs: break\n",
    "\n",
    "      # Display loss\n",
    "      plt.plot(LOSS)\n",
    "      plt.ylabel(\"Loss\")\n",
    "      plt.xlabel(\"Epochs\")\n",
    "      plt.title(\"Evolution of the loss through epochs\")\n",
    "\n",
    "  main_bottom()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOOvhqtzpKXj"
   },
   "source": [
    "### Random generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-09T08:28:47.958313Z",
     "iopub.status.busy": "2022-03-09T08:28:47.957959Z",
     "iopub.status.idle": "2022-03-09T08:28:51.396240Z",
     "shell.execute_reply": "2022-03-09T08:28:51.395066Z",
     "shell.execute_reply.started": "2022-03-09T08:28:47.958259Z"
    },
    "id": "MuzAGzVbpNP-",
    "outputId": "d7d47f3c-4864-4af8-c667-cb2b2d41e4d2"
   },
   "outputs": [],
   "source": [
    "if generation_model==\"transformer\":\n",
    "    GENERATOR_PATH = '/generator/logs/version_1/checkpoints/epoch=18.ckpt'\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    #DEVICE = 'cpu'\n",
    "\n",
    "    def random_generate_from_GPT2(generator, path_generator=GENERATOR_PATH, path_vqvae=VAE_PATH, nb_examples=2):\n",
    "        vae = make_vae()\n",
    "        vae.load_state_dict(torch.load(path_vqvae, map_location='cuda'))\n",
    "        vae.to(DEVICE)\n",
    "        vae.eval()\n",
    "\n",
    "        #generator = Generator.load_from_checkpoint(path_generator)\n",
    "        generator = cudafy(generator)\n",
    "        #generator.eval()\n",
    "\n",
    "        #input_ids = cudafy(torch.randint(0,generator.hparams.vocab_size-1, (nb_examples**2, 1)).long())\n",
    "        result_idxs = generator.generate(nb_examples=nb_examples)\n",
    "        print(\"result_idxs\", result_idxs.shape)\n",
    "        print(\"max(result_idxs)\", torch.max(result_idxs))\n",
    "        #result_idxs[result_idxs>=generator.hparams.vocab_size-1] = generator.hparams.vocab_size-2\n",
    "        result_idxs[result_idxs>=generator.hparams.vocab_size] = generator.hparams.vocab_size-1\n",
    "        print(\"max(result_idxs)\", torch.max(result_idxs))\n",
    "        bottom_idxs = result_idxs[:,:16,:]\n",
    "        print(\"bottom_idxs\", bottom_idxs.shape)\n",
    "        top_idxs = result_idxs[:,16:,:]\n",
    "        print(\"top_idxs\", top_idxs.shape)\n",
    "        top_idxs = top_idxs.view(len(top_idxs), -1)\n",
    "        print(\"top_idxs\", top_idxs.shape)\n",
    "        top_idxs = top_idxs.reshape((len(top_idxs),8,8))\n",
    "        print(\"top_idxs\", top_idxs.shape)\n",
    "        bottom_generated = vae.encoders[0].vq.embed(bottom_idxs)\n",
    "        top_generated = vae.encoders[1].vq.embed(top_idxs)\n",
    "        print(\"bottom_generated\", bottom_generated.shape)\n",
    "        print(\"top_generated\", top_generated.shape)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated = vae.decoders[1]([top_generated,bottom_generated])\n",
    "            #generated = vae.decoders[0]([torch.rand(nb_examples,64,8,8)])\n",
    "            print(\"generated.shape\", generated.shape)\n",
    "            #generated = vae.reconstruct_from_code(result)\n",
    "\n",
    "            fig, axes = plt.subplots(1,2, figsize=(25,25))\n",
    "            img = make_grid(generated, nb_examples).permute(1,2,0)*0.1352 + 0.3541\n",
    "            axes[0].imshow(cudafy.get(img).detach().cpu().numpy())\n",
    "            axes[0].set_title('Generated from the sampled codes')\n",
    "            vae.eval()\n",
    "\n",
    "            #recon = vae(generated)[\"reconstructions\"]\n",
    "            #img = make_grid(recon, nb_examples).permute(1,2,0)*0.1352 + 0.3541\n",
    "            #axes[1].imshow(cudafy.get(img).detach().cpu().numpy())\n",
    "            #axes[1].set_title('Reconstructed from the generated images')\n",
    "\n",
    "    random_generate_from_GPT2(generator, nb_examples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-03-09T08:28:51.397481Z",
     "iopub.status.idle": "2022-03-09T08:28:51.398316Z",
     "shell.execute_reply": "2022-03-09T08:28:51.398069Z",
     "shell.execute_reply.started": "2022-03-09T08:28:51.398034Z"
    },
    "id": "XWBA8rN0Bkoz",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate samples using the top-level prior.\n",
    "\"\"\"\n",
    "\n",
    "if generation_model==\"pixelCNN\":\n",
    "  NUM_SAMPLES = 4\n",
    "\n",
    "  def main_sample():\n",
    "      device = torch.device(DEVICE)\n",
    "\n",
    "      vae = make_vae()\n",
    "      vae.load_state_dict(torch.load(VAE_PATH))\n",
    "      vae.to(device)\n",
    "      vae.eval()\n",
    "\n",
    "      top_prior = TopPrior()\n",
    "      top_prior.load_state_dict(torch.load(TOP_PRIOR_PATH))\n",
    "      top_prior.to(device)\n",
    "\n",
    "      results = np.zeros([NUM_SAMPLES, 8, 8], dtype=np.long)\n",
    "      for row in range(results.shape[1]):\n",
    "          for col in range(results.shape[2]):\n",
    "              partial_in = torch.from_numpy(results[:, :row + 1]).to(device)\n",
    "              with torch.no_grad():\n",
    "                  outputs = torch.softmax(top_prior(partial_in), dim=1).cpu().numpy()\n",
    "              for i, out in enumerate(outputs):\n",
    "                  probs = out[:, row, col]\n",
    "                  results[i, row, col] = sample_softmax(probs)\n",
    "          print('done row', row)\n",
    "      with torch.no_grad():\n",
    "          full_latents = torch.from_numpy(results).to(device)\n",
    "          top_embedded = vae.encoders[1].vq.embed(full_latents)\n",
    "          bottom_encoded = vae.decoders[0]([top_embedded])\n",
    "          bottom_embedded, _, _ = vae.encoders[0].vq(bottom_encoded)\n",
    "          decoded = torch.clamp(vae.decoders[1]([top_embedded, bottom_embedded]), 0, 1)\n",
    "      decoded = decoded.permute(0, 2, 3, 1).cpu().numpy()\n",
    "      decoded = np.concatenate(decoded, axis=1)\n",
    "      Image.fromarray((decoded * 255).astype(np.uint8)).save('top_samples.png')\n",
    "\n",
    "\n",
    "  def sample_softmax(probs):\n",
    "      number = random.random()\n",
    "      for i, x in enumerate(probs):\n",
    "          number -= x\n",
    "          if number <= 0:\n",
    "              return i\n",
    "      return len(probs) - 1\n",
    "\n",
    "\n",
    "  def arg_parser():\n",
    "      parser = argparse.ArgumentParser()\n",
    "      parser.add_argument('--device', help='torch device', default='cuda')\n",
    "      return parser\n",
    "\n",
    "  main_sample()\n",
    "  samples = Image.open('top_samples.png')\n",
    "  samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
